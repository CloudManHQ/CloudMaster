# 强化学习基础 (RL Foundations)
> **一句话理解**: 强化学习就像训练小狗做动作——做对了给零食（奖励），做错了不给（惩罚），小狗通过反复试错学会什么行为能获得最多零食。

## 1. 概述 (Overview)

强化学习（Reinforcement Learning, RL）是机器学习的第三大范式，与监督学习、无监督学习并列。它通过**智能体（Agent）与环境（Environment）的交互**，通过试错（Trial and Error）的方式，学习在给定状态下采取何种动作以最大化长期累积奖励。

### 1.1 RL的独特性
- **延迟奖励（Delayed Reward）**: 动作的好坏可能在很久之后才显现（如围棋中的落子）
- **探索与利用的权衡（Exploration vs Exploitation）**: 需要在尝试新动作和利用已知最优动作之间平衡
- **序贯决策（Sequential Decision Making）**: 当前决策会影响未来状态，需要长期规划

### 1.2 RL vs 监督学习 vs 无监督学习

| 对比维度 | 监督学习 (Supervised) | 无监督学习 (Unsupervised) | 强化学习 (Reinforcement) |
|---------|---------------------|--------------------------|-------------------------|
| 训练数据 | 标注数据 (X, Y) | 无标注数据 X | 环境交互产生的(状态,动作,奖励)序列 |
| 学习目标 | 学习从X到Y的映射函数 | 发现数据的内在结构/分布 | 学习最优策略以最大化累积奖励 |
| 反馈机制 | 每个样本都有明确标签 | 无明确反馈 | 稀疏的奖励信号，可能有延迟 |
| 数据分布 | i.i.d（独立同分布） | i.i.d | 非i.i.d（时序相关） |
| 典型应用 | 图像分类、语音识别 | 聚类、降维 | 游戏AI、机器人控制、推荐系统 |
| 错误代价 | 立即反馈错误 | 无对错概念 | 错误可能导致灾难性后果 |

### 1.3 RL的经典应用
- **游戏AI**: AlphaGo、Dota 2的OpenAI Five、星际争霸的AlphaStar
- **机器人控制**: 波士顿动力的机器人步态控制
- **推荐系统**: YouTube、淘宝的强化学习推荐
- **自动驾驶**: 路径规划和决策
- **资源调度**: 数据中心冷却系统优化（Google DeepMind）

## 2. 核心概念 (Core Concepts)

### 2.1 马尔可夫决策过程 (Markov Decision Process, MDP)

MDP是强化学习的数学基础，由五元组定义：**⟨S, A, P, R, γ⟩**

| 要素 | 符号 | 含义 | 示例（Grid World） |
|-----|------|------|--------------------|
| 状态空间 | S | 所有可能状态的集合 | 网格中的所有位置 |
| 动作空间 | A | 智能体可执行的动作集合 | {上, 下, 左, 右} |
| 转移概率 | P(s'│s,a) | 在状态s执行动作a后转移到s'的概率 | 90%按指令移动，10%滑倒 |
| 奖励函数 | R(s,a,s') | 状态转移时获得的即时奖励 | 到达目标+10，掉入陷阱-10 |
| 折扣因子 | γ ∈ [0,1] | 未来奖励的衰减系数 | 0.9（更重视眼前利益） |

**马尔可夫性质（Markov Property）**: 未来状态只依赖于当前状态，与历史无关：
```
P[S_t+1 | S_t, A_t, S_t-1, A_t-1, ...] = P[S_t+1 | S_t, A_t]
```

**状态转移示意图（ASCII）**:
```
    +-------+     a1      +-------+     a2      +-------+
    |  S_t  | ---------> |  S_t+1| ---------> |  S_t+2|
    +-------+            +-------+            +-------+
       |                    |                    |
       v R_t               v R_t+1              v R_t+2
```

### 2.2 策略 (Policy)

策略 π 定义了在每个状态下选择动作的规则：

- **确定性策略**: `a = π(s)` （在状态s下必定选择动作a）
- **随机策略**: `π(a|s) = P(A_t=a | S_t=s)` （在状态s下以一定概率选择动作a）

**示例**：在迷宫中，策略可以是"遇到墙壁就左转"，或者"70%向目标前进，30%随机探索"。

### 2.3 值函数 (Value Function)

值函数用于评估状态或动作的好坏：

**状态值函数 V^π(s)**: 从状态s开始，遵循策略π能获得的期望累积奖励
```
V^π(s) = E_π[G_t | S_t = s] = E_π[Σ_{k=0}^∞ γ^k R_{t+k+1} | S_t = s]
```

**动作值函数 Q^π(s,a)**: 在状态s执行动作a后，遵循策略π能获得的期望累积奖励
```
Q^π(s,a) = E_π[G_t | S_t = s, A_t = a]
```

**关系**: `V^π(s) = Σ_a π(a|s) Q^π(s,a)`

**最优值函数**:
```
V*(s) = max_π V^π(s)
Q*(s,a) = max_π Q^π(s,a)
```

### 2.4 贝尔曼方程 (Bellman Equation)

贝尔曼方程是RL的核心数学工具，建立了当前状态值与未来状态值的递归关系。

#### 贝尔曼期望方程（Bellman Expectation Equation）

**状态值函数的贝尔曼方程**:
```
V^π(s) = Σ_a π(a|s) Σ_{s'} P(s'|s,a) [R(s,a,s') + γ V^π(s')]
```

**推导过程**:
```
V^π(s) = E_π[G_t | S_t = s]
       = E_π[R_{t+1} + γ G_{t+1} | S_t = s]
       = E_π[R_{t+1} + γ V^π(S_{t+1}) | S_t = s]
       = Σ_a π(a|s) Σ_{s'} P(s'|s,a) [R(s,a,s') + γ V^π(s')]
```

**动作值函数的贝尔曼方程**:
```
Q^π(s,a) = Σ_{s'} P(s'|s,a) [R(s,a,s') + γ Σ_{a'} π(a'|s') Q^π(s',a')]
```

#### 贝尔曼最优方程（Bellman Optimality Equation）

```
V*(s) = max_a Σ_{s'} P(s'|s,a) [R(s,a,s') + γ V*(s')]
Q*(s,a) = Σ_{s'} P(s'|s,a) [R(s,a,s') + γ max_{a'} Q*(s',a')]
```

**直觉理解**: 最优值等于"即时奖励 + 折扣后的未来最优值"。

**贝尔曼更新示意图**:
```
        V(s) 
         |
    +----+----+
    |    |    |
   a1   a2   a3  (所有可能动作)
    |    |    |
   s1'  s2'  s3' (可能到达的状态)
    |    |    |
  V(s1') ...  V(s3') (未来状态的值)
  
V(s) = max_a [R(s,a) + γ Σ P(s'|s,a) V(s')]
```

### 2.5 探索与利用权衡 (Exploration-Exploitation Tradeoff)

这是RL中最核心的挑战之一：
- **利用（Exploitation）**: 选择当前已知最优的动作，获取已知的高奖励
- **探索（Exploration）**: 尝试新动作，可能发现更优策略

#### 常用探索策略

**1. ε-贪心策略 (ε-Greedy)**
```
π(a|s) = {
    1 - ε + ε/|A|,  if a = argmax_a' Q(s,a')  (最优动作)
    ε/|A|,          otherwise                  (其他动作)
}
```
以概率 ε 随机探索，1-ε 选择最优动作。通常 ε 随训练递减（如从1.0衰减至0.01）。

**2. Upper Confidence Bound (UCB)**
```
a_t = argmax_a [Q(s,a) + c √(ln(t) / N(s,a))]
```
其中 N(s,a) 是动作a在状态s下被选择的次数。UCB会自动平衡：
- Q(s,a) 高的动作（利用）
- N(s,a) 低的动作（探索未知）

**3. Thompson Sampling**
对每个动作的Q值维护一个概率分布，每次从分布中采样并选择采样值最大的动作。适用于贝叶斯强化学习。

**对比表**:
| 策略 | 优点 | 缺点 | 适用场景 |
|-----|------|------|---------|
| ε-Greedy | 简单易实现 | 盲目随机探索，效率低 | 小规模问题 |
| UCB | 理论保证强，探索更智能 | 需要统计动作次数 | 多臂老虎机问题 |
| Thompson Sampling | 贝叶斯框架，适应性强 | 计算复杂度高 | 推荐系统、在线实验 |

## 3. 关键算法/技术详解 (Key Algorithms/Techniques)

### 3.1 动态规划 (Dynamic Programming)

适用于**已知环境模型**（P和R）的情况。

#### 策略迭代 (Policy Iteration)
```
1. 初始化策略 π_0
2. 重复直到收敛：
   a) 策略评估：解线性方程组得到 V^π(s)
   b) 策略改进：π'(s) = argmax_a Q^π(s,a)
```

#### 价值迭代 (Value Iteration)
```
1. 初始化 V(s) = 0 for all s
2. 重复直到收敛：
   V(s) ← max_a Σ_{s'} P(s'|s,a) [R(s,a,s') + γ V(s')]
3. 提取策略：π(s) = argmax_a Q(s,a)
```

**复杂度**: 对于 |S| 个状态和 |A| 个动作，时间复杂度为 O(|S|² |A|)。

### 3.2 蒙特卡洛方法 (Monte Carlo Methods)

适用于**环境模型未知**但可以**采样完整轨迹**的情况。

#### 基本思想
通过**完整episode**的实际回报来估计值函数：
```
V(S_t) ← V(S_t) + α [G_t - V(S_t)]
```
其中 G_t = R_{t+1} + γ R_{t+2} + γ² R_{t+3} + ... 是实际累积奖励。

#### First-Visit vs Every-Visit
- **First-Visit MC**: 每个episode中只在状态s第一次出现时更新V(s)
- **Every-Visit MC**: 每次访问状态s都更新V(s)

**优点**: 不需要环境模型，可以从经验学习  
**缺点**: 必须等episode结束才能更新，不适用于持续任务

### 3.3 时序差分学习 (Temporal Difference Learning)

TD学习是MC和DP的结合，能够**在线学习**且不需要环境模型。

#### TD(0) 更新规则
```
V(S_t) ← V(S_t) + α [R_{t+1} + γ V(S_{t+1}) - V(S_t)]
                      └────────┬─────────┘
                          TD目标
                      └─────────────┬──────────────┘
                              TD误差 (δ_t)
```

**TD目标**: R_{t+1} + γ V(S_{t+1}) —— 使用估计值替代真实累积奖励  
**TD误差**: δ_t = R_{t+1} + γ V(S_{t+1}) - V(S_t) —— 衡量预测误差

#### TD vs MC 对比

| 维度 | MC | TD |
|------|----|----|
| 更新时机 | Episode结束 | 每一步 |
| 自举(Bootstrapping) | 否（用真实回报） | 是（用估计值） |
| 方差 | 高 | 低 |
| 偏差 | 无偏 | 有偏（初期） |
| 收敛速度 | 慢 | 快 |
| 适用任务 | 分幕式 | 分幕式+持续式 |

**直觉**: TD学习像"在线考试实时反馈"，MC像"期末考试一次性反馈"。

### 3.4 Q-Learning (Off-Policy TD Control)

Q-Learning是**无模型、离线策略**的经典算法，学习最优动作值函数 Q*(s,a)。

#### Q-Learning 算法伪代码
```
初始化 Q(s,a) 任意值（通常为0），对终止状态 Q(terminal,·) = 0
对每个episode:
    初始化状态 S
    对episode中的每一步:
        用策略（如ε-贪心）从 Q 导出动作 A
        执行 A，观察 R, S'
        Q(S,A) ← Q(S,A) + α [R + γ max_{a'} Q(S',a') - Q(S,A)]
        S ← S'
    直到 S 是终止状态
```

**关键特性**:
- **Off-Policy**: 行为策略（ε-贪心）和目标策略（贪心）不同
- **Bootstrap**: 使用 max_{a'} Q(S',a') 估计未来价值
- **收敛性**: 在满足访问所有(s,a)对无限次且学习率递减条件下，Q收敛到Q*

#### Q-Learning 更新直觉图
```
当前状态 S, 动作 A
     |
     v (执行动作)
下一状态 S', 奖励 R
     |
     v (选择最优动作)
max_{a'} Q(S', a') ← 用于计算TD目标
     |
     v (更新)
Q(S,A) ← Q(S,A) + α [R + γ max_{a'} Q(S',a') - Q(S,A)]
```

### 3.5 SARSA (On-Policy TD Control)

与Q-Learning的唯一区别是**实际执行的下一个动作**用于更新：
```
Q(S,A) ← Q(S,A) + α [R + γ Q(S',A') - Q(S,A)]
                            └──┬──┘
                        实际选择的动作A'
```

**On-Policy**: 评估和改进的是同一个策略。

**Q-Learning vs SARSA**:
- Q-Learning更激进（learn the optimal policy），适合离线学习
- SARSA更保守（learn the policy being executed），在探索阶段更安全

## 4. 代码实战 (Hands-on Code)

### 4.1 Grid World 环境下的 Q-Learning 实现

```python
import numpy as np
import gymnasium as gym

# 创建Grid World环境（Gymnasium的FrozenLake）
env = gym.make('FrozenLake-v1', is_slippery=False)  # 确定性环境
n_states = env.observation_space.n
n_actions = env.action_space.n

# 初始化Q表
Q = np.zeros((n_states, n_actions))

# 超参数
alpha = 0.1          # 学习率
gamma = 0.99         # 折扣因子
epsilon = 1.0        # 初始探索率
epsilon_decay = 0.995
epsilon_min = 0.01
n_episodes = 2000

# ε-贪心策略
def epsilon_greedy(state, epsilon):
    if np.random.random() < epsilon:
        return env.action_space.sample()  # 探索
    else:
        return np.argmax(Q[state])        # 利用

# Q-Learning 训练
for episode in range(n_episodes):
    state, _ = env.reset()
    done = False
    total_reward = 0
    
    while not done:
        # 选择动作
        action = epsilon_greedy(state, epsilon)
        
        # 执行动作
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        # Q-Learning 更新
        td_target = reward + gamma * np.max(Q[next_state])
        td_error = td_target - Q[state, action]
        Q[state, action] += alpha * td_error
        
        state = next_state
        total_reward += reward
    
    # 衰减探索率
    epsilon = max(epsilon_min, epsilon * epsilon_decay)
    
    if (episode + 1) % 200 == 0:
        print(f"Episode {episode+1}, Epsilon: {epsilon:.3f}")

# 测试学到的策略
state, _ = env.reset()
env.render()
done = False
while not done:
    action = np.argmax(Q[state])
    state, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated
    env.render()

env.close()
```

### 4.2 CartPole 问题的表格型 Q-Learning

```python
import gymnasium as gym
import numpy as np

env = gym.make('CartPole-v1')

# 离散化连续状态空间
def discretize_state(state):
    # CartPole状态: [cart_pos, cart_vel, pole_angle, pole_vel]
    bins = [
        np.linspace(-2.4, 2.4, 10),    # cart position
        np.linspace(-3, 3, 10),         # cart velocity
        np.linspace(-0.5, 0.5, 10),     # pole angle
        np.linspace(-2, 2, 10)          # pole velocity
    ]
    discrete_state = tuple([np.digitize(state[i], bins[i]) for i in range(4)])
    return discrete_state

# Q表（10^4个状态，2个动作）
Q = np.zeros([10] * 4 + [2])

# 训练参数
alpha, gamma, epsilon = 0.1, 0.99, 1.0
episodes = 5000

for ep in range(episodes):
    state, _ = env.reset()
    state = discretize_state(state)
    done = False
    
    while not done:
        # ε-贪心
        if np.random.random() < epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(Q[state])
        
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        next_state = discretize_state(next_state)
        
        # Q-Learning更新
        Q[state][action] += alpha * (
            reward + gamma * np.max(Q[next_state]) - Q[state][action]
        )
        
        state = next_state
    
    epsilon = max(0.01, epsilon * 0.995)

env.close()
print("训练完成！Q表已学习最优策略。")
```

## 5. 应用场景与案例 (Applications & Cases)

### 5.1 游戏AI
- **AlphaGo**: 蒙特卡洛树搜索 + 深度强化学习，2016年击败李世石
- **Dota 2 OpenAI Five**: 多智能体协作，256个GPU训练10个月
- **星际争霸 AlphaStar**: 处理超大状态空间（10^26），达到大师级水平

### 5.2 机器人控制
- **波士顿动力**: 四足机器人步态控制使用RL实现复杂地形导航
- **抓取任务**: Google的机器人通过自我学习掌握抓取多种物体

### 5.3 推荐系统
- **YouTube推荐**: 将推荐建模为序贯决策，最大化长期用户参与度
- **淘宝推荐**: 使用Q-Learning优化推荐策略，提升GMV

### 5.4 资源调度
- **Google数据中心**: 使用RL优化冷却系统，节省40%能源
- **交通信号灯控制**: 实时调整红绿灯时长，减少拥堵

### 5.5 金融交易
- **量化交易**: RL学习买卖时机，但需注意过拟合和市场变化

## 6. 进阶话题 (Advanced Topics)

### 6.1 奖励稀疏问题 (Sparse Reward)

**挑战**: 很多任务只有在最终成功时才有奖励（如走迷宫），中间过程无反馈。

**解决方案**:
- **奖励塑造 (Reward Shaping)**: 人为设计中间奖励（但要避免引入偏差）
- **课程学习 (Curriculum Learning)**: 从简单任务逐步过渡到困难任务
- **分层强化学习 (Hierarchical RL)**: 将任务分解为子目标
- **逆强化学习 (Inverse RL)**: 从专家演示中学习奖励函数
- **好奇心驱动 (Curiosity-Driven)**: 内在奖励机制，鼓励探索新颖状态

### 6.2 On-Policy vs Off-Policy

| 维度 | On-Policy | Off-Policy |
|------|-----------|------------|
| 定义 | 评估和改进同一策略 | 评估目标策略，但用行为策略采样 |
| 数据效率 | 低（需实时采样） | 高（可重用历史数据） |
| 稳定性 | 更稳定 | 容易发散 |
| 典型算法 | SARSA, A3C, PPO | Q-Learning, DQN, SAC |
| 适用场景 | 在线学习，安全探索 | 离线学习，数据受限 |

### 6.3 信用分配问题 (Credit Assignment)

**问题**: 在长序列中，如何判断哪些动作对最终结果起了关键作用？

**方法**:
- **n-步回报**: 平衡TD(0)和MC的优势
- **资格迹 (Eligibility Traces)**: TD(λ) 算法，指数衰减历史贡献
- **优势函数 (Advantage Function)**: A(s,a) = Q(s,a) - V(s)，衡量动作相对平均水平的优势

### 6.4 常见陷阱与调试技巧

**陷阱1: 奖励设计不当**
- 例子：让机器人"快速到达目标"，结果学会了原地打转刷速度奖励
- 解决：仔细检查奖励函数，避免意外的奖励漏洞

**陷阱2: 探索不足**
- 症状：智能体卡在次优策略
- 解决：增加探索率，使用更智能的探索策略（UCB, Thompson Sampling）

**陷阱3: 学习率不当**
- 太大：震荡不收敛；太小：学习太慢
- 解决：使用学习率衰减，或自适应优化器（Adam）

**调试技巧**:
1. **先用简单环境测试**（如CartPole）确保算法正确
2. **可视化Q值/价值函数**，观察是否合理
3. **记录平均回报曲线**，应平滑上升
4. **设置随机种子**，确保可复现

### 6.5 RL的前沿方向
- **Offline RL**: 从固定数据集学习，无需在线交互（BCQ, CQL）
- **Meta-RL**: 学会快速适应新任务（MAML, RL²）
- **Safe RL**: 引入约束，避免危险动作
- **Multi-Agent RL**: 多智能体博弈与协作
- **Model-Based RL**: 学习环境模型以提高样本效率（World Models, MuZero）

## 7. 与其他主题的关联 (Connections)

### 7.1 前置知识
- **数学基础**:
  - [概率统计](../../01_Fundamentals/Probability_Statistics/Probability_Statistics.md): 期望、方差、条件概率
  - [线性代数](../../01_Fundamentals/Linear_Algebra/Linear_Algebra.md): 矩阵运算、特征分解
  - [微积分](../../01_Fundamentals/Linear_Algebra/Linear_Algebra.md): 梯度、链式法则

- **机器学习基础**:
  - [监督学习](../../02_Machine_Learning/Supervised_Learning/Supervised_Learning.md): 理解损失函数、梯度下降
  - [优化方法](../../03_Deep_Learning/Optimization/Optimization.md): SGD, Adam等优化器

### 7.2 后续进阶
- **深度强化学习**: [Deep RL](../Deep_RL/Deep_RL.md) —— DQN, PPO, SAC等现代算法
- **AI智能体**: [AI Agents](../AI_Agents/AI_Agents.md) —— 基于LLM的自主规划系统
- **多智能体系统**: 博弈论、协作学习
- **模仿学习**: 从人类演示中学习（Behavior Cloning, GAIL）

### 7.3 相关领域
- **最优控制理论**: RL是离散时间的随机最优控制
- **运筹学**: 动态规划在RL中的应用
- **认知科学**: RL模型与人类学习机制的关联

## 8. 面试高频问题 (Interview FAQs)

### Q1: RL与监督学习的本质区别是什么？
**A**: 
- **反馈类型**: 监督学习有明确的正确答案（标签），RL只有标量奖励信号，且可能延迟
- **数据分布**: 监督学习假设数据i.i.d，RL中状态序列时序相关
- **目标**: 监督学习学习映射函数，RL学习策略以最大化长期累积奖励
- **探索需求**: RL需要主动探索环境，监督学习被动接收数据

### Q2: 解释探索-利用困境及解决方案
**A**: 
**困境**: 智能体必须在"利用已知最优动作获取奖励"和"探索未知动作发现更优策略"之间权衡。纯利用会陷入局部最优，纯探索无法利用已有知识。

**解决方案**:
- **ε-贪心**: 以概率ε随机探索，通常ε随训练递减
- **UCB**: 选择"平均奖励+不确定性奖励"最大的动作，鼓励探索不确定区域
- **Thompson Sampling**: 贝叶斯方法，从后验分布采样
- **内在奖励**: 给予探索新颖状态额外奖励

### Q3: 贝尔曼方程的意义是什么？
**A**: 贝尔曼方程建立了值函数的**递归结构**，是RL的数学基础：
- **意义**: 将无限时间步的累积奖励问题转化为一步奖励+未来价值的递推
- **应用**: 
  - 贝尔曼期望方程用于策略评估
  - 贝尔曼最优方程用于求解最优策略
  - 是动态规划、TD学习的理论基础
- **直觉**: "一个状态的价值等于即时奖励加上未来状态的折扣价值"

### Q4: Q-Learning为何是Off-Policy？有何优缺点？
**A**: 
**Off-Policy定义**: 评估的目标策略（贪心策略）与生成数据的行为策略（ε-贪心）不同。

**Q-Learning的Off-Policy性质**: 更新时使用 `max_{a'} Q(S',a')`，即假设未来遵循贪心策略，但实际执行的是ε-贪心。

**优点**:
- **数据效率高**: 可重用历史经验（经验回放）
- **学习最优策略**: 即使行为策略不是最优的
- **灵活性**: 可以从人类专家或其他策略的数据中学习

**缺点**:
- **高方差**: Off-Policy导致更新方差大，可能不稳定
- **需要重要性采样**: 理论上需要修正分布偏差（虽然Q-Learning不显式使用）

### Q5: 如何处理RL中的奖励稀疏问题？
**A**: 
**问题**: 很多任务只有在最终成功时才有奖励，导致学习困难。

**解决方案**:
1. **奖励塑造 (Reward Shaping)**: 
   - 设计中间奖励（如"距离目标更近+0.1"）
   - 风险：可能引入偏差，导致学习错误策略
   - 保证：基于势函数的塑造可保证最优策略不变

2. **分层强化学习 (Hierarchical RL)**: 
   - 将任务分解为多个子目标
   - 高层策略选择子目标，低层策略执行

3. **课程学习 (Curriculum Learning)**: 
   - 从简单任务开始，逐步增加难度

4. **模仿学习**: 
   - 先用专家演示进行监督学习，再用RL微调

5. **内在奖励 (Intrinsic Motivation)**: 
   - 好奇心驱动（Curiosity-Driven）：奖励预测误差
   - 计数法（Count-Based）：奖励访问稀少的状态

6. **后见之明经验回放 (Hindsight Experience Replay, HER)**: 
   - 将失败轨迹的终点重新标记为"假想目标"，增加正样本

## 9. 参考资源 (References)

### 9.1 经典教材
- **Sutton & Barto - Reinforcement Learning: An Introduction (2nd Edition)**: RL圣经，[免费在线阅读](http://incompleteideas.net/book/the-book-2nd.html)
- **David Silver的UCL课程**: [YouTube播放列表](https://www.youtube.com/watch?v=2pWv7GOvuf0)

### 9.2 论文
- **Q-Learning**: Watkins, C. J., & Dayan, P. (1992). Q-learning. Machine learning.
- **TD-Gammon**: Tesauro, G. (1995). Temporal difference learning and TD-Gammon. Communications of the ACM.
- **AlphaGo**: Silver et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature.

### 9.3 开源库与环境
- **Gymnasium**: OpenAI Gym的继任者，标准RL环境库 - [https://gymnasium.farama.org/](https://gymnasium.farama.org/)
- **Stable-Baselines3**: PyTorch实现的SOTA算法库 - [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)
- **RLlib (Ray)**: 分布式RL框架 - [https://docs.ray.io/en/latest/rllib/](https://docs.ray.io/en/latest/rllib/)

### 9.4 在线资源
- **OpenAI Spinning Up**: 深度RL教程 - [https://spinningup.openai.com/](https://spinningup.openai.com/)
- **DeepMind x UCL讲座系列**: [YouTube](https://www.youtube.com/c/DeepMind)
- **李宏毅强化学习课程**: [YouTube中文字幕版](https://www.youtube.com/watch?v=z95ZYgPgXOY)

### 9.5 实践项目
- **CleanRL**: 单文件RL算法实现 - [https://github.com/vwxyzjn/cleanrl](https://github.com/vwxyzjn/cleanrl)
- **MiniGrid**: 简化的网格世界环境 - [https://minigrid.farama.org/](https://minigrid.farama.org/)
- **RL Adventure 系列博客**: [https://github.com/higgsfield/RL-Adventure](https://github.com/higgsfield/RL-Adventure)

---
*Last updated: 2026-02-10*
