# 图像分类与检测 (Image Classification & Detection)

> **一句话理解**: 图像分类就像"看图识物"——告诉计算机这是猫还是狗;目标检测则更进一步,不仅要识别"是什么",还要圈出"在哪里",就像给照片中的每个物体画边框并标注名字。

## 1. 概述 (Overview)

图像分类和目标检测是计算机视觉 (Computer Vision, CV) 的两大基础任务:
- **图像分类 (Image Classification)**: 给定整张图像,预测其类别 (如猫、狗、汽车)
- **目标检测 (Object Detection)**: 定位图像中所有物体的位置 (边界框) 并分类

这两个任务是众多 CV 应用的基石,包括自动驾驶、医疗影像、安防监控、工业质检等。

### 发展历程

```
传统时期 (2012 年前):
  - 手工特征 (SIFT, HOG) + 分类器 (SVM)
  - 准确率低,泛化能力弱

深度学习革命:
  2012: AlexNet (ImageNet 冠军) - CNN 时代开启
  2014: VGG, GoogLeNet - 网络加深
  2015: ResNet - 残差连接解决梯度消失
  2017: MobileNet - 轻量化模型
  2019: EfficientNet - 复合缩放
  2020: Vision Transformer (ViT) - Transformer 入侵 CV
  2023: DINOv2, SAM - 自监督学习 + 通用分割

目标检测演进:
  2014: R-CNN - 两阶段检测器
  2015: Fast R-CNN, Faster R-CNN
  2016: YOLO v1, SSD - 一阶段检测器
  2020: DETR - Transformer 检测
  2023: YOLOv8, YOLOv10 - 实时检测 SOTA
```

---

## 2. 核心概念 (Core Concepts)

### 2.1 卷积神经网络 (CNN) 基础

#### 卷积操作直觉

卷积本质是**滑动窗口特征提取**,通过可学习的滤波器 (Filter/Kernel) 检测局部模式。

```
输入图像 (5×5)          卷积核 (3×3)         输出特征图 (3×3)
┌─────────────┐       ┌───────┐          ┌─────────┐
│ 1  2  3  4  5│       │ 1  0 -1│          │ -8  -8  -8│
│ 6  7  8  9 10│   *   │ 1  0 -1│   =      │ -8  -8  -8│
│11 12 13 14 15│       │ 1  0 -1│          │ -8  -8  -8│
│16 17 18 19 20│       └───────┘          └─────────┘
│21 22 23 24 25│       (边缘检测滤波器)    (检测到竖直边缘)
└─────────────┘
```

**滑动窗口过程**:
```
Step 1: 核对齐左上角 3×3 区域
┌───────┐
│ 1  2  3│
│ 6  7  8│ * Filter → 输出[0, 0] = -8
│11 12 13│
└───────┘

Step 2: 向右滑动一格 (stride=1)
   ┌───────┐
   │ 2  3  4│
   │ 7  8  9│ * Filter → 输出[0, 1] = -8
   │12 13 14│
   └───────┘
```

#### CNN 的层次化特征提取

```
输入图像
   ↓
低层卷积层: 检测边缘、纹理、颜色
   │  (3×3, 7×7 卷积核)
   ↓
中层卷积层: 检测局部形状 (圆形、角等)
   │  (组合低层特征)
   ↓
高层卷积层: 检测物体部件 (眼睛、轮子等)
   │
   ↓
全连接层: 整合全局信息,分类
   ↓
输出: 类别概率
```

### 2.2 经典 CNN 架构演进对比

| 模型 | 年份 | 核心创新 | 深度 | ImageNet Top-5 错误率 | 参数量 | 特点 |
|------|------|---------|------|-----------------------|--------|------|
| **AlexNet** | 2012 | ReLU, Dropout, 数据增强 | 8 层 | 16.4% | 61M | 开启深度学习时代 |
| **VGG-16** | 2014 | 统一 3×3 小卷积核 | 16 层 | 7.3% | 138M | 结构简洁,广泛应用 |
| **GoogLeNet (Inception v1)** | 2014 | Inception 模块,多尺度 | 22 层 | 6.7% | 7M | 参数高效 |
| **ResNet-50** | 2015 | **残差连接** (跳跃连接) | 50 层 | 3.6% | 25M | 可训练超深网络 (1000层+) |
| **DenseNet** | 2017 | 密集连接 (每层连接所有前层) | 121 层 | 3.5% | 8M | 参数效率极高 |
| **MobileNet v2** | 2018 | 深度可分离卷积 | - | 8.7% | 3.5M | 移动端部署 |
| **EfficientNet-B7** | 2019 | 复合缩放 (深度+宽度+分辨率) | - | 2.3% | 66M | 精度与效率平衡 SOTA |
| **Vision Transformer (ViT)** | 2020 | 纯 Transformer,Patch Embedding | 12 层 | 2.0% | 86M | 大数据下超越 CNN |

### 2.3 残差块 (Residual Block) 详解

**问题**: 网络加深后,训练变困难 (梯度消失/爆炸),甚至出现退化 (训练误差上升)。

**解决方案**: 残差连接 (Shortcut Connection) 让梯度直接回传。

#### 残差块结构

```
        Input x
          │
    ┌─────┴─────┐
    │           │ 跳跃连接 (Identity Mapping)
    │           │
    ▼           │
  Conv 3×3      │
    │           │
    ▼           │
  ReLU          │
    │           │
    ▼           │
  Conv 3×3      │
    │           │
    ▼           │
  ┌─┴───────────┘
  │ 加法 (Add)
  ▼
 ReLU
  │
Output: F(x) + x
```

**数学表达**:
$$
y = F(x, \{W_i\}) + x
$$

- $F(x)$: 残差映射 (需要学习的部分)
- $x$: 恒等映射 (直接跳过)

**为什么有效?**
- 如果恒等映射是最优的,$F(x)$ 只需学到 0 即可
- 梯度可直接通过加法回传,缓解梯度消失

### 2.4 Vision Transformer (ViT) 原理

ViT 将图像视为序列,直接应用 Transformer。

#### Patch Embedding 过程

```
原始图像 (224×224×3)
         ↓ 切分成 Patches (16×16)
┌──┬──┬──┬──┐
│P1│P2│P3│P4│  每个 Patch: 16×16×3 = 768 维
├──┼──┼──┼──┤  展平后: (224/16)² = 196 个 Patches
│P5│P6│P7│P8│
├──┼──┼──┼──┤
│...  ...  │
└──┴──┴──┴──┘
         ↓ 线性投影 (768 → d_model)
  Patch Embeddings (196 × d_model)
         ↓ 添加位置编码
         ↓ 标准 Transformer Encoder
         ↓ [CLS] Token 输出 → 分类
```

**关键组件**:
1. **Patch Embedding**: 将图像切块并线性投影
2. **Position Embedding**: 1D 可学习位置编码
3. **[CLS] Token**: 类似 BERT,用于分类任务

**ViT vs CNN**:
| 维度 | CNN | ViT |
|------|-----|-----|
| **归纳偏置** | 强 (局部性、平移不变性) | 弱 (需要大数据学习) |
| **小数据表现** | 优 | 差 |
| **大数据表现** | 好 | **更优** |
| **计算复杂度** | O(HWC²k²) | O(N²d) (N=Patch数) |
| **可解释性** | 特征图可视化 | Attention 热力图 |

---

## 3. 关键算法/技术详解 (Key Algorithms/Techniques)

### 3.1 目标检测: YOLO 系列演进

YOLO (You Only Look Once) 将检测问题转化为**单次回归任务**,极速且准确。

#### YOLO 核心思想

```
传统两阶段 (Faster R-CNN):
  1. Region Proposal (候选框生成)
  2. 分类 + 边界框回归
  → 慢 (两次前向传播)

YOLO 一阶段:
  输入图像 → CNN → 直接预测 (类别 + 边界框)
  → 快 (一次前向传播)
```

#### YOLO 输出结构

```
输入图像 (640×640)
      ↓ Backbone (如 CSPDarknet)
  特征图 (20×20×C, 40×40×C, 80×80×C)  ← 多尺度
      ↓ Detection Head
每个网格预测:
  - Bounding Box 坐标 (x, y, w, h) × K 个 Anchor
  - 置信度 (Objectness Score)
  - 类别概率 (C 个类别)
```

#### YOLO 版本对比

| 版本 | 年份 | 关键改进 | 速度 (FPS) | 精度 (mAP) |
|------|------|---------|-----------|-----------|
| **YOLOv1** | 2016 | 首次单阶段实时检测 | 45 | 63.4 |
| **YOLOv3** | 2018 | 多尺度预测,残差网络 | 30 | 70.0 |
| **YOLOv5** | 2020 | 自适应 Anchor,数据增强 | 140 | 75.0 |
| **YOLOv7** | 2022 | E-ELAN, 重参数化 | 160 | 78.0 |
| **YOLOv8** | 2023 | Anchor-Free, 简化 Head | 200+ | 80.0 |
| **YOLOv10** | 2024 | NMS-Free,端到端 | 220+ | 81.5 |

**YOLOv8 新特性**:
- **Anchor-Free**: 不再使用预设 Anchor,直接回归边界框
- **解耦头 (Decoupled Head)**: 分类和定位分支独立
- **数据增强**: Mosaic, MixUp, CopyPaste

### 3.2 目标检测评估指标

#### IoU (Intersection over Union)

衡量预测框和真实框的重叠度:

$$
\text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}
$$

```
Ground Truth Box      Predicted Box
┌────────┐            ┌────────┐
│        │            │   ┌────┼───┐
│    GT  │            │   │ O  │ P │  O: Overlap (交集)
│        │            │   └────┼───┘
└────────┘            └────────┘
         Union = GT + P - O
```

- IoU > 0.5: 通常认为是正确检测
- IoU > 0.7: 高质量检测

#### mAP (mean Average Precision)

目标检测的核心指标,综合考虑精确率和召回率。

**计算步骤**:
1. 对每个类别,按置信度排序所有预测
2. 计算不同召回率下的精确率
3. 绘制 Precision-Recall 曲线,计算曲线下面积 (AP)
4. 对所有类别求平均 → mAP

**常见变体**:
- **mAP@0.5**: IoU 阈值 0.5 时的 mAP
- **mAP@0.5:0.95**: IoU 从 0.5 到 0.95 (步长 0.05) 的平均 mAP (COCO 标准)

### 3.3 两阶段 vs 一阶段检测器对比

| 维度 | 两阶段 (Faster R-CNN) | 一阶段 (YOLO) |
|------|----------------------|---------------|
| **流程** | Region Proposal + 分类 | 直接回归 |
| **速度** | 慢 (10-30 FPS) | 快 (100+ FPS) |
| **精度** | 高 (适合高精度场景) | 中高 (YOLOv8 已接近) |
| **小物体检测** | 优 | 较弱 (但在改进) |
| **实时性** | 不适合 | 适合 |

---

## 4. 代码实战 (Hands-on Code)

### 4.1 ResNet 残差块 PyTorch 实现

```python
import torch
import torch.nn as nn

class BasicBlock(nn.Module):
    """ResNet 基础残差块 (用于 ResNet-18/34)"""
    expansion = 1  # 输出通道数倍增因子
    
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        # 主路径
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                               stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # 跳跃连接 (如果维度不匹配,需要投影)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, 
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        identity = x  # 保存输入
        
        # 主路径
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        # 残差连接
        out += self.shortcut(identity)
        out = self.relu(out)
        
        return out

class ResNet18(nn.Module):
    """简化版 ResNet-18"""
    def __init__(self, num_classes=1000):
        super().__init__()
        self.in_channels = 64
        
        # 初始卷积层
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # 残差块组
        self.layer1 = self._make_layer(64, 2, stride=1)
        self.layer2 = self._make_layer(128, 2, stride=2)
        self.layer3 = self._make_layer(256, 2, stride=2)
        self.layer4 = self._make_layer(512, 2, stride=2)
        
        # 分类头
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
    
    def _make_layer(self, out_channels, num_blocks, stride):
        """构建残差块组"""
        layers = []
        # 第一个块可能需要下采样
        layers.append(BasicBlock(self.in_channels, out_channels, stride))
        self.in_channels = out_channels
        
        # 后续块
        for _ in range(1, num_blocks):
            layers.append(BasicBlock(out_channels, out_channels, stride=1))
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # 输入: (batch, 3, 224, 224)
        x = self.conv1(x)      # (batch, 64, 112, 112)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)    # (batch, 64, 56, 56)
        
        x = self.layer1(x)     # (batch, 64, 56, 56)
        x = self.layer2(x)     # (batch, 128, 28, 28)
        x = self.layer3(x)     # (batch, 256, 14, 14)
        x = self.layer4(x)     # (batch, 512, 7, 7)
        
        x = self.avgpool(x)    # (batch, 512, 1, 1)
        x = torch.flatten(x, 1)  # (batch, 512)
        x = self.fc(x)         # (batch, num_classes)
        
        return x

# 测试
if __name__ == "__main__":
    model = ResNet18(num_classes=10)
    x = torch.randn(2, 3, 224, 224)
    output = model(x)
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
```

### 4.2 Ultralytics YOLOv8 推理代码

```python
from ultralytics import YOLO
import cv2

# ========== 1. 加载预训练模型 ==========
model = YOLO('yolov8n.pt')  # nano 模型 (最快)
# 其他选项: yolov8s.pt (small), yolov8m.pt (medium), yolov8l.pt (large), yolov8x.pt (xlarge)

# ========== 2. 图像检测 ==========
results = model('bus.jpg')  # 图像路径

# 处理结果
for result in results:
    boxes = result.boxes  # 边界框
    for box in boxes:
        # 提取信息
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()  # 坐标
        confidence = box.conf[0].cpu().numpy()  # 置信度
        class_id = int(box.cls[0].cpu().numpy())  # 类别 ID
        class_name = model.names[class_id]  # 类别名称
        
        print(f"{class_name} ({confidence:.2f}) at [{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]")
    
    # 可视化
    annotated_img = result.plot()  # 绘制边界框
    cv2.imshow('YOLOv8 Detection', annotated_img)
    cv2.waitKey(0)

# ========== 3. 视频检测 (实时) ==========
cap = cv2.VideoCapture(0)  # 0 = 摄像头, 或视频路径

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    
    # 检测
    results = model(frame, verbose=False)
    annotated_frame = results[0].plot()
    
    # 显示
    cv2.imshow('YOLOv8 Real-time', annotated_frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

# ========== 4. 自定义训练 ==========
# 准备数据集 (YOLO 格式)
# dataset/
#   ├── images/
#   │   ├── train/
#   │   └── val/
#   ├── labels/
#   │   ├── train/  (每个图像对应一个 .txt 标注文件)
#   │   └── val/
#   └── data.yaml

# 训练
model = YOLO('yolov8n.pt')  # 加载预训练权重
results = model.train(
    data='dataset/data.yaml',
    epochs=100,
    imgsz=640,
    batch=16,
    device=0  # GPU 0
)

# ========== 5. 导出为 ONNX (部署) ==========
model.export(format='onnx')  # 生成 yolov8n.onnx
```

**输出示例**:
```
person (0.92) at [120, 50, 300, 400]
bus (0.88) at [400, 100, 800, 500]
car (0.75) at [50, 300, 150, 380]
```

---

## 5. 应用场景与案例 (Applications & Cases)

### 5.1 自动驾驶 (Autonomous Driving)
- **任务**: 检测行人、车辆、交通标志、车道线
- **模型**: YOLOv8, Faster R-CNN
- **挑战**: 实时性 (60+ FPS)、极端天气鲁棒性

### 5.2 医疗影像分析
- **任务**: X 光/CT 病灶检测、肿瘤分类
- **模型**: ResNet, EfficientNet (分类), Mask R-CNN (分割+检测)
- **挑战**: 小样本、高精度要求、可解释性

### 5.3 工业质检
- **任务**: 表面缺陷检测 (划痕、裂纹)
- **模型**: YOLOv8 (实时检测)
- **优势**: 7×24 小时工作,一致性高

### 5.4 零售分析
- **任务**: 货架商品识别、顾客行为分析
- **模型**: YOLOv5/v8
- **应用**: 无人超市、智能收银

### 5.5 安防监控
- **任务**: 异常行为检测、人脸识别
- **模型**: YOLOv8 (检测) + RetinaFace (人脸)

---

## 6. 进阶话题 (Advanced Topics)

### 6.1 数据增强技巧

| 方法 | 原理 | 适用场景 |
|------|------|---------|
| **Mosaic** | 将 4 张图像拼接成 1 张 | YOLOv5/v8 标配 |
| **MixUp** | 两张图像加权混合 | 提升泛化能力 |
| **CutMix** | 裁剪一张图像贴到另一张 | 防止过拟合 |
| **AutoAugment** | 强化学习搜索最优增强策略 | 大数据集 |
| **RandAugment** | 随机采样增强操作 | 简单高效 |

### 6.2 轻量化模型技术

- **深度可分离卷积 (Depthwise Separable Convolution)**: 
  - 标准卷积: $C_{in} \times C_{out} \times K^2$
  - 可分离: $C_{in} \times K^2 + C_{in} \times C_{out}$ (参数减少 8-9×)
- **知识蒸馏 (Knowledge Distillation)**: 大模型指导小模型训练
- **网络剪枝 (Pruning)**: 移除冗余通道/层

### 6.3 常见陷阱

1. **类别不平衡**: 长尾类别样本少,模型偏向主类
   - **解决**: Focal Loss, 重采样, 数据增强
2. **过拟合小数据集**: 训练集很小时模型记忆训练样本
   - **解决**: 数据增强, 迁移学习, 正则化
3. **边界框回归不准**: IoU 低
   - **解决**: DIoU/CIoU Loss (考虑边界框距离和长宽比)

---

## 7. 与其他主题的关联 (Connections)

### 前置知识
- [卷积神经网络基础](../../01_Fundamentals/Deep_Learning_Basics/Deep_Learning_Basics.md)
- [优化算法](../../01_Fundamentals/Optimization/Optimization.md): SGD, Adam

### 后续推荐
- [图像分割](../Segmentation/Segmentation.md): 语义分割、实例分割
- [生成模型](../Generative_Models/Generative_Models.md): GAN, Diffusion
- [多模态视觉](../Multimodal_Vision/Multimodal_Vision.md): CLIP, BLIP

### 跨领域应用
- [迁移学习](../../02_Machine_Learning/Transfer_Learning/Transfer_Learning.md): 预训练模型微调
- [模型评估](../../07_AI_Engineering/Model_Evaluation/Model_Evaluation.md): 混淆矩阵、ROC 曲线

---

## 8. 面试高频问题 (Interview FAQs)

### Q1: 为什么 ResNet 能训练超深网络?

**答**: 残差连接解决了两大问题:
1. **梯度消失**: 梯度可通过加法直接回传,避免多次乘法导致的衰减
2. **退化问题**: 即使某些层学不到有用特征,至少可以学到恒等映射 (输出=输入)

**数学直觉**:
- 普通网络: $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$ (链式法则,多层相乘→梯度消失)
- ResNet: $y = F(x) + x$ → $\frac{\partial y}{\partial x} = \frac{\partial F}{\partial x} + 1$ (总有 1 保底,梯度不会消失)

### Q2: ViT 和 CNN 的本质区别?

| 维度 | CNN | ViT |
|------|-----|-----|
| **归纳偏置** | 强 (局部性、平移不变性) | 弱 (需从数据学习) |
| **小数据 (<1M)** | 优 | 差 (欠拟合) |
| **大数据 (>10M)** | 好 | 更优 |
| **计算复杂度** | 局部卷积 (高效) | 全局 Attention (平方复杂度) |
| **长距离依赖** | 需堆叠多层 | 一层即可捕获 |

**何时选择**:
- CNN: 数据量中等 (<10M)、移动端部署、需要局部特征
- ViT: 大数据集 (ImageNet-21K+)、追求 SOTA 精度

### Q3: 为什么 YOLO 比 Faster R-CNN 快?

**答**: 架构差异:
- **Faster R-CNN**: 两阶段
  1. RPN 生成 ~2000 个候选框 (Region Proposals)
  2. 对每个框进行分类和边界框回归
  → 两次前向传播,大量冗余计算

- **YOLO**: 一阶段
  - 单次前向传播,网格直接回归边界框
  → 无候选框生成,端到端

**速度对比**:
- Faster R-CNN: ~10 FPS
- YOLOv8: ~200 FPS (A100 GPU)

**代价**: YOLO 早期版本对小物体检测较弱,但 v8 已大幅改进。

### Q4: 如何选择目标检测模型?

| 场景 | 推荐模型 | 原因 |
|------|---------|------|
| **实时应用** (自动驾驶) | YOLOv8/v10 | 速度快 (100+ FPS) |
| **高精度需求** (医疗) | Faster R-CNN, Cascade R-CNN | mAP 更高 |
| **移动端部署** | YOLOv8n, MobileNet-SSD | 参数量小 |
| **视频分析** | YOLOv8 + 跟踪算法 (DeepSORT) | 平衡速度与精度 |

### Q5: mAP@0.5 和 mAP@0.5:0.95 有什么区别?

**答**:
- **mAP@0.5**: IoU 阈值 0.5 时的 mAP
  - 宽松标准,重叠度 >50% 即算正确
  - PASCAL VOC 竞赛标准

- **mAP@0.5:0.95**: IoU 从 0.5 到 0.95 (步长 0.05) 的平均
  - 严格标准,考察定位精度
  - COCO 竞赛标准

**为什么 COCO 用 0.5:0.95?**
- 鼓励精确定位 (不仅要检测到,还要框得准)
- 更能体现模型真实水平

---

## 9. 参考资源 (References)

### 论文
- [Deep Residual Learning for Image Recognition (ResNet)](https://arxiv.org/abs/1512.03385)
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)](https://arxiv.org/abs/2010.11929)
- [You Only Look Once: Unified, Real-Time Object Detection (YOLO v1)](https://arxiv.org/abs/1506.02640)
- [YOLOv8 Technical Report](https://docs.ultralytics.com/)
- [Faster R-CNN: Towards Real-Time Object Detection](https://arxiv.org/abs/1506.01497)

### 开源项目
- [Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics) - 工业级检测框架
- [torchvision](https://pytorch.org/vision/stable/index.html) - PyTorch 官方 CV 库
- [MMDetection](https://github.com/open-mmlab/mmdetection) - OpenMMLab 检测工具箱
- [Detectron2](https://github.com/facebookresearch/detectron2) - Facebook 检测库

### 数据集
- [ImageNet](https://www.image-net.org/) - 1400 万图像,1000 类
- [COCO](https://cocodataset.org/) - 目标检测/分割标准数据集
- [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) - 经典检测数据集
- [Open Images V7](https://storage.googleapis.com/openimages/web/index.html) - 900 万图像,600 类

### 教程
- [Stanford CS231n: Convolutional Neural Networks](http://cs231n.stanford.edu/)
- [Papers with Code - Object Detection](https://paperswithcode.com/task/object-detection)
- [YOLOv8 官方文档](https://docs.ultralytics.com/)

---

*Last updated: 2026-02-10*
