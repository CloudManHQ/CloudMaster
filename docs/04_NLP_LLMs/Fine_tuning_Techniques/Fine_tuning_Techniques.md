# 微调技术 (Fine-tuning Techniques)

> **一句话理解**: 微调就像给已经受过通识教育的大学生进行"专业培训"——不是从零教起(预训练已完成),而是针对特定技能(下游任务)进行强化训练,既省时间又保留了原有知识。

## 1. 概述 (Overview)

预训练模型 (如 GPT, BERT) 在海量数据上学习了通用的语言知识,但对特定任务 (如客服对话、医疗问答) 往往表现不佳。**微调 (Fine-tuning)** 通过在目标任务数据上继续训练,使模型适应具体场景。

### 微调发展历程

```
2018: 全参数微调 (Full Fine-tuning) - BERT, GPT
      问题: 每个任务需要存储完整模型副本

2019: Adapter - 只训练小模块,插入预训练层之间
      问题: 推理时增加额外计算开销

2021: Prompt Tuning - 只训练输入层的 Soft Prompt
      问题: 小模型效果差,需要超大模型 (10B+)

2021: LoRA - 低秩适配,冻结原模型,训练轻量级旁路
      突破: 显存低、速度快、效果好

2023: QLoRA - LoRA + 4-bit 量化
      突破: 单张 24GB GPU 微调 65B 模型

2023: RLHF/DPO - 基于人类反馈的对齐
      突破: 让模型输出符合人类偏好 (ChatGPT 核心技术)

2024: ORPO/KTO - 无需奖励模型的对齐
      方向: 简化对齐流程,降低训练成本
```

### 微调方法分类

```
                      Fine-tuning 方法
                            │
            ┌───────────────┼───────────────┐
            │               │               │
        全参数微调       参数高效微调     强化学习对齐
      (Full FT)         (PEFT)          (Alignment)
            │               │               │
      更新所有参数      只训练少量参数      基于反馈优化
      显存需求高        显存需求低        需要奖励信号
            │               │               │
      ┌─────┴─────┐   ┌─────┼─────┐   ┌─────┴─────┐
  Supervised    Instruction  LoRA   Adapter  RLHF    DPO
  Fine-tuning   Tuning      QLoRA  Prefix-T  PPO    ORPO
```

---

## 2. 核心概念 (Core Concepts)

### 2.1 全参数微调 vs 参数高效微调 (PEFT) 对比

| 维度 | 全参数微调 (Full Fine-tuning) | PEFT (如 LoRA) |
|------|-------------------------------|----------------|
| **可训练参数** | 100% (所有参数) | 0.1-1% (仅适配器) |
| **显存占用** | 与预训练相同 (需梯度+优化器状态) | 10-30% 的全参微调 |
| **训练速度** | 慢 (反向传播所有层) | 快 (仅更新少量参数) |
| **效果** | 最优 (理论上限) | 接近全参微调 (90-95%) |
| **多任务切换** | 需存储多个模型副本 | 只存储小适配器 (~10MB) |
| **适用场景** | 数据充足、资源充足 | 资源受限、多任务部署 |

**显存对比示例** (LLaMA-7B, AdamW 优化器):
- 全参微调: ~28 GB (模型 14GB + 梯度 14GB + 优化器状态 42GB, ZeRO-2 优化)
- LoRA (r=8): ~10 GB (原模型只读 14GB + LoRA 参数 <100MB + 梯度/优化器)

### 2.2 LoRA: 低秩适配详解

#### 核心思想

冻结预训练权重 $W_0 \in \mathbb{R}^{d \times k}$,通过低秩分解矩阵学习增量:

$$
W = W_0 + \Delta W = W_0 + BA
$$

其中:
- $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$
- $r \ll \min(d, k)$ (秩,通常 4-64)
- 可训练参数: $r(d+k)$ vs 原始 $d \times k$

#### 可视化原理

```
原始矩阵 W (d × k)           LoRA 低秩分解
┌─────────────────┐         ┌──────┐  ┌──────┐
│                 │         │      │  │      │
│   d × k 参数     │   ≈     │ B    │  │  A   │
│   (全部训练)     │         │(d×r) │ ×│(r×k) │
│                 │         │      │  │      │
└─────────────────┘         └──────┘  └──────┘
   10M 参数                  5K 参数 + 5K 参数
 (假设 d=1000, k=10000)     (假设 r=10)
                             参数量减少 99%!
```

**为什么低秩分解有效?**

经验发现,微调时权重更新矩阵 $\Delta W$ 的**内在秩 (Intrinsic Rank) 很低**,即大部分信息集中在少数几个主方向,不需要全秩矩阵。

#### LoRA 应用位置

通常应用于 Attention 层的 Q, K, V, O 投影矩阵:

```
      Input x
         │
         ├──────┐
         │      │ 冻结路径
         ▼      ▼
       W₀x    B(Ax)  ← LoRA 旁路 (可训练)
         │      │
         └──┬───┘
            ▼
      Output (W₀ + BA)x
```

**前向传播**:
```python
output = W0 @ x + (B @ A) @ x  # 可合并为 (W0 + BA) @ x
```

**推理优化**: 可将 $W_0 + BA$ 合并成单个矩阵,无额外计算开销!

#### 关键超参数: 秩 r 的选择

| 秩 r | 参数量 | 效果 | 适用场景 |
|------|--------|------|---------|
| r = 1-2 | 极少 | 较差 | 简单任务、实验 |
| r = 4-8 | 少 | **优秀** (推荐) | 通用场景 |
| r = 16-32 | 中 | 接近全参 | 复杂任务、大模型 |
| r = 64-128 | 较多 | ~全参效果 | 极限性能需求 |

**经验法则**: 先从 r=8 开始,如不够再增大。

### 2.3 QLoRA: 4-bit 量化 + LoRA

QLoRA 通过将基础模型量化到 4-bit,进一步压缩显存,使得**单张消费级 GPU 微调 65B 模型**成为可能。

#### 核心技术栈

```
┌─────────────────────────────────────┐
│  QLoRA = NF4 量化 + LoRA + 其他优化 │
└─────────────────┬───────────────────┘
                  │
    ┌─────────────┼─────────────┐
    ▼             ▼             ▼
 NF4 量化      Double Dequant  Paged Optimizers
 (4-bit)       (避免精度损失)  (CPU-GPU 换页)
    │             │             │
将模型压缩      计算时动态反量化  优化器状态放 CPU
到 25% 大小      到 BF16       节省显存峰值
```

#### NF4 (4-bit NormalFloat) 量化原理

标准量化将权重映射到均匀分布的离散值,但神经网络权重通常服从**正态分布**。NF4 设计了针对正态分布优化的量化表:

```
标准 INT4 (均匀):  -8, -7, -6, ..., 0, ..., 6, 7
NF4 (信息论最优):  密集覆盖 [-1, 1],稀疏覆盖尾部
```

**效果**: 相同 4-bit 下,NF4 精度损失更小。

#### 显存对比

以 LLaMA-65B 为例:
- BF16 全参微调: ~780 GB (需 10×A100 80GB)
- QLoRA (4-bit + r=64): ~48 GB (**单张 A100 或 RTX 4090 24GB × 2**)

---

## 3. 关键算法/技术详解 (Key Algorithms/Techniques)

### 3.1 其他 PEFT 方法对比

| 方法 | 原理 | 优点 | 缺点 | 推理开销 |
|------|------|------|------|---------|
| **Adapter** | 在 Transformer 层中插入小型 FFN 模块 | 简单,易实现 | 增加推理延迟 (串行计算) | ✗ 有 |
| **Prefix Tuning** | 在输入序列前添加可学习的虚拟 token | 无推理开销 | 占用上下文长度 | ✓ 无 |
| **P-Tuning v2** | 在每层添加可学习的 Prompt | 效果好 | 实现复杂 | ✓ 无 |
| **LoRA** | 低秩矩阵分解 | **平衡最优** | 需要矩阵合并步骤 | ✓ 无 (合并后) |
| **(IA)³** | 学习激活值的缩放向量 | 参数极少 (0.01%) | 效果略逊于 LoRA | ✓ 无 |

**推荐选择**: LoRA 在效果、显存、速度三方面平衡最优,是当前主流。

### 3.2 RLHF: 基于人类反馈的强化学习

RLHF (Reinforcement Learning from Human Feedback) 是 ChatGPT 的核心技术,用于让模型输出符合人类偏好。

#### 三阶段完整流程

```
阶段 1: 监督微调 (SFT)
┌──────────────────────────────┐
│  预训练模型 + 高质量示例      │
│  "问: ..., 答: ..."          │
└──────────┬───────────────────┘
           ▼
      SFT 模型 (初步对齐)


阶段 2: 奖励模型训练 (Reward Model)
┌──────────────────────────────┐
│  人类标注偏好对比数据         │
│  Response A > Response B     │
└──────────┬───────────────────┘
           ▼
   奖励模型 RM(x, y) → score
   (预测人类偏好得分)


阶段 3: 强化学习优化 (PPO)
┌──────────────────────────────┐
│  SFT 模型 生成回复            │
│  RM 打分 → PPO 算法优化       │
│  最大化奖励,同时限制偏离 SFT   │
└──────────┬───────────────────┘
           ▼
    最终对齐模型 (ChatGPT)
```

#### 奖励模型训练

给定 Prompt x, 两个回复 $y_w$ (更好), $y_l$ (更差),训练 RM 最大化:

$$
\mathcal{L}_{RM} = -\mathbb{E}\left[\log \sigma\left(r(x, y_w) - r(x, y_l)\right)\right]
$$

其中 $r(x, y)$ 是奖励模型输出的标量分数。

#### PPO 优化目标

$$
\mathcal{L}_{PPO} = \mathbb{E}\left[ r(x, y) - \beta \cdot D_{KL}(\pi_\theta || \pi_{SFT}) \right]
$$

- $r(x, y)$: 奖励模型打分
- $\beta \cdot D_{KL}$: KL 散度惩罚项 (防止过度偏离 SFT 模型)

**挑战**:
- 训练不稳定 (RL 固有问题)
- 需要 4 个模型同时运行 (SFT, RM, PPO, Ref),显存开销大
- 超参数敏感

### 3.3 DPO: 直接偏好优化

DPO (Direct Preference Optimization) 绕过奖励模型,直接优化偏好数据,简化 RLHF 流程。

#### 核心洞察

RLHF 的 RM + PPO 可以被重新参数化为**直接最大化偏好数据的似然**:

$$
\mathcal{L}_{DPO} = -\mathbb{E}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]
$$

**简化理解**: 增大 $y_w$ 的概率,降低 $y_l$ 的概率,同时控制偏离参考模型的程度。

#### DPO vs RLHF 对比

| 维度 | RLHF (PPO) | DPO |
|------|-----------|-----|
| **流程复杂度** | 3 阶段 (SFT→RM→PPO) | 1 阶段 (直接优化) |
| **所需模型** | 4 个 (SFT, RM, Policy, Ref) | 2 个 (Policy, Ref) |
| **训练稳定性** | 不稳定 (RL 固有) | 稳定 (监督学习) |
| **显存需求** | 极高 | 中等 |
| **效果** | SOTA | 接近 RLHF (某些任务超越) |

### 3.4 其他对齐方法

| 方法 | 核心思想 | 优点 | 缺点 |
|------|---------|------|------|
| **ORPO** (Odds Ratio PO) | 单阶段,SFT + 偏好优化同步 | 无需 SFT 预训练 | 需大量偏好数据 |
| **KTO** (Kahneman-Tversky Optimization) | 基于人类行为经济学设计损失 | 只需二元反馈 (好/坏) | 理论较新,实践少 |
| **IPO** (Identity PO) | 避免 DPO 的梯度消失问题 | 训练更稳定 | 效果提升有限 |

---

## 4. 代码实战 (Hands-on Code)

### 使用 Hugging Face PEFT 库实现 LoRA 微调

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset

# ========== 1. 加载预训练模型 ==========
model_name = "meta-llama/Llama-2-7b-hf"  # 或其他模型
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Llama 没有 pad token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # 使用 BF16 节省显存
    device_map="auto"  # 自动分配到 GPU
)

# ========== 2. 配置 LoRA ==========
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,  # 任务类型
    r=8,  # 秩 (rank)
    lora_alpha=32,  # 缩放因子 (通常设为 r 的 2-4 倍)
    lora_dropout=0.1,  # Dropout 概率
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # 应用 LoRA 的模块
    bias="none"  # 不训练 bias
)

# 包装模型
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# 输出: trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%

# ========== 3. 准备数据集 ==========
dataset = load_dataset("tatsu-lab/alpaca", split="train[:1000]")  # 示例: Alpaca 指令数据

def preprocess_function(examples):
    """将数据格式化为 Instruction-Input-Output"""
    prompts = []
    for inst, inp, out in zip(examples["instruction"], examples["input"], examples["output"]):
        if inp:
            prompt = f"### Instruction:\n{inst}\n\n### Input:\n{inp}\n\n### Response:\n{out}"
        else:
            prompt = f"### Instruction:\n{inst}\n\n### Response:\n{out}"
        prompts.append(prompt)
    
    # Tokenize
    tokenized = tokenizer(
        prompts,
        truncation=True,
        max_length=512,
        padding="max_length"
    )
    tokenized["labels"] = tokenized["input_ids"].copy()  # 自回归任务
    return tokenized

tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)

# ========== 4. 训练配置 ==========
training_args = TrainingArguments(
    output_dir="./lora_llama2_alpaca",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,  # 有效 batch size = 16
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=False,
    bf16=True,  # 使用 BF16
    logging_steps=10,
    save_strategy="epoch",
    optim="adamw_torch",  # 优化器
    report_to="none"  # 关闭 wandb
)

# ========== 5. 开始训练 ==========
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

trainer.train()

# ========== 6. 保存 LoRA 权重 ==========
model.save_pretrained("./lora_weights")  # 只保存 LoRA 参数 (~10MB)

# ========== 7. 推理测试 ==========
model.eval()
prompt = "### Instruction:\nWrite a Python function to calculate factorial.\n\n### Response:\n"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        temperature=0.7,
        top_p=0.9
    )

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

**输出示例**:
```python
### Instruction:
Write a Python function to calculate factorial.

### Response:
def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n - 1)
```

### QLoRA 微调 (4-bit 量化)

只需修改加载模型部分:

```python
from transformers import BitsAndBytesConfig

# 4-bit 量化配置
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,  # Double Quantization
    bnb_4bit_quant_type="nf4",  # NormalFloat4
    bnb_4bit_compute_dtype=torch.bfloat16  # 计算时用 BF16
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

# 其余代码不变,LoRA 配置需添加:
lora_config = LoraConfig(
    ...,
    task_type=TaskType.CAUSAL_LM,
)
```

**显存对比**:
- 不量化: ~28 GB
- QLoRA (4-bit): ~10 GB (可在 RTX 3090/4090 24GB 运行)

---

## 5. 应用场景与案例 (Applications & Cases)

### 5.1 指令微调 (Instruction Tuning)
- **目标**: 让模型理解并遵循指令
- **数据**: Alpaca, Dolly, FLAN 等指令数据集
- **方法**: LoRA/QLoRA 微调

### 5.2 对话系统定制
- **场景**: 企业客服、医疗咨询
- **流程**: SFT (示例对话) → DPO (人类反馈)

### 5.3 领域适配
- **医疗**: 在 PubMed 文献上微调 LLaMA
- **法律**: 在法律文书数据上微调
- **代码**: 在特定编程语言代码库上微调

### 5.4 多任务学习
- **方法**: 为每个任务训练独立的 LoRA 适配器
- **优势**: 共享基础模型,按需加载适配器 (~10MB)

---

## 6. 进阶话题 (Advanced Topics)

### 6.1 LoRA 的理论理解

**为什么低秩有效?**
- 研究表明,微调时的权重更新矩阵 $\Delta W$ 实际上低秩
- Aghajanyan et al. (2021) 发现"内在维度"远小于参数空间

**LoRA 的局限**:
- 对需要大幅改变知识的任务 (如新领域预训练) 效果不佳
- 秩 r 的选择需要实验

### 6.2 对齐的常见陷阱

1. **过度对齐 (Over-alignment)**: 模型变得过于谨慎,拒绝回答正常问题
2. **奖励 Hacking**: PPO 可能学会"欺骗"奖励模型而非真正提升质量
3. **偏好数据偏差**: 标注者的偏见会被模型学习

### 6.3 前沿方向

- **参数合并 (Model Merging)**: 将多个 LoRA 适配器合并到单个模型
- **稀疏微调**: 只更新最重要的参数子集
- **元学习微调**: 学习如何快速适配新任务

---

## 7. 与其他主题的关联 (Connections)

### 前置知识
- [Transformer 革命](../Transformer_Revolution/Transformer_Revolution.md): 理解模型架构
- [大语言模型架构](../LLM_Architectures/LLM_Architectures.md): GPT/LLaMA 原理
- [优化算法](../../01_Fundamentals/Optimization/Optimization.md): Adam, SGD

### 后续推荐
- [提示工程](../Prompt_Engineering/Prompt_Engineering.md): 微调的替代方案
- [模型评估](../../07_AI_Engineering/Model_Evaluation/Model_Evaluation.md): 如何评估微调效果
- [量化技术](../../07_AI_Engineering/Model_Compression/Model_Compression.md): INT8/INT4 量化原理

---

## 8. 面试高频问题 (Interview FAQs)

### Q1: LoRA 的秩 r 如何选择?

**答**: 
- **通用建议**: 先从 r=8 开始,90% 场景够用
- **实验策略**: 尝试 [4, 8, 16] 三个值,选效果最好的
- **任务相关**:
  - 简单任务 (情感分析): r=4 即可
  - 复杂任务 (代码生成): r=16-32
  - 大模型 (70B+): 可增大到 r=64

**理论依据**: 秩 r 控制适配能力,越大越接近全参微调,但参数量和训练时间也增加。

### Q2: DPO 相比 RLHF 的优势?

| 维度 | RLHF | DPO |
|------|------|-----|
| **流程** | 3 阶段,需训练 RM | 1 阶段,直接优化 |
| **稳定性** | RL 训练不稳定 | 稳定 (监督学习) |
| **显存** | 需同时运行 4 个模型 | 只需 2 个模型 |
| **实现复杂度** | 高 (PPO 调参难) | 低 (标准 Cross-Entropy) |
| **效果** | SOTA | 接近或相当 |

**结论**: DPO 是工程上更优的选择,RLHF 理论上限更高但实践困难。

### Q3: 为什么微调后模型可能"遗忘"预训练知识?

**现象**: 在特定任务上微调后,通用能力下降。

**原因**:
1. **灾难性遗忘 (Catastrophic Forgetting)**: 新任务数据覆盖了原有知识
2. **分布偏移**: 微调数据与预训练分布差异大

**解决方案**:
- 小学习率 (1e-5 ~ 2e-4)
- 混入通用数据 (如 10% C4 数据)
- 使用 PEFT 方法 (LoRA 破坏性更小)
- Elastic Weight Consolidation (EWC) 正则化

### Q4: 全参微调和 LoRA 什么时候效果相当?

**答**: 
- **数据量小** (<10K 样本): LoRA 通常足够
- **数据量大** (>100K): 全参微调可能更优
- **任务与预训练相似**: LoRA 够用
- **任务差异大** (如代码→自然语言): 全参微调更好

**经验**: 70% 场景 LoRA (r=8-16) 效果达全参微调 95%+。

### Q5: 如何评估微调效果?

**自动指标**:
- 任务特定指标 (分类准确率、BLEU、ROUGE 等)
- 困惑度 (Perplexity)

**人工评估**:
- 人类标注偏好对比
- A/B Testing

**通用能力保持**:
- 在基准测试 (如 MMLU, BBH) 上评估
- 确保微调没有破坏原有能力

---

## 9. 参考资源 (References)

### 论文
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/abs/2203.02155)
- [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290)
- [P-Tuning v2](https://arxiv.org/abs/2110.07602)

### 开源库
- [Hugging Face PEFT](https://github.com/huggingface/peft) - LoRA/Adapter/Prefix Tuning 等
- [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl) - RLHF/DPO 实现
- [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) - 微调工具链
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) - 一站式微调框架

### 数据集
- [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) - 52K 指令数据
- [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) - 15K 高质量指令
- [FLAN Collection](https://github.com/google-research/FLAN) - 多任务指令数据

### 教程
- [Hugging Face Fine-tuning Guide](https://huggingface.co/docs/transformers/training)
- [QLoRA Paper Walkthrough](https://www.youtube.com/watch?v=TPcXVJ1VSRI)
- [DPO Tutorial by Hugging Face](https://huggingface.co/blog/dpo-trl)

---

*Last updated: 2026-02-10*
