# 04 自然语言处理与大模型 (NLP & LLMs)

## 4.1 Transformer 革命

### [Self-Attention 机制与架构](./Transformer_Revolution/Transformer_Revolution.md)
- **核心**: 多头注意力、位置编码、残差连接。

## 4.2 大语言模型 (LLMs)

### [预训练范式与主流架构](./LLM_Architectures/LLM_Architectures.md)
- **内容**: GPT (Decoder-only), BERT (Encoder-only), MoE (混合专家模型)。

## 4.3 微调技术 (Fine-tuning)

### [参数高效微调 (PEFT)](./Fine_tuning_Techniques/Fine_tuning_Techniques.md)
- **算法**: LoRA, QLoRA, Prefix Tuning。

---
*Last updated: 2026-02-08*
